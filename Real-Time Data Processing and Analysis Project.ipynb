{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Real-Time Data Processing and Analysis Project**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In this project, we aim to build a robust system for real-time processing and analysis of structured and unstructured job-related data. The system is designed to handle various data formats, including text, JSON, CSV, Excel, PDF, and images, which are ingested from multiple sources. Using Apache Spark, a powerful distributed data processing framework, we process, clean, and transform the data into actionable insights.\n",
    "\n",
    "The data processed in this project includes job bulletins, positions, salaries, requirements, and other related fields. By leveraging Sparkâ€™s real-time streaming capabilities, this pipeline handles live data streams, ensuring efficient and scalable processing for dynamic datasets. Furthermore, the processed data is stored in cloud-based systems, making it readily available for downstream analytics, visualization, and business decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## **Motivation**\n",
    "\n",
    "The motivation behind this project lies in the increasing demand for real-time data processing in industries such as recruitment, business intelligence, and automation. Manual data extraction and analysis are time-consuming and prone to errors. With this automated pipeline:\n",
    "- Businesses can streamline their data workflows, reduce human intervention, and ensure consistency.\n",
    "- Job applicants and recruiters gain immediate insights into job postings, enabling faster decision-making.\n",
    "- The system serves as a proof-of-concept for scalable, real-time, multi-format data pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works**\n",
    "\n",
    "This project follows a **modular architecture** that includes the following key steps:\n",
    "\n",
    "1. **Data Ingestion**:\n",
    "   - Data from multiple sources (text, JSON, CSV, PDF, Excel, and images) is ingested into the system using a unified framework powered by Spark Streaming and custom extractors.\n",
    "\n",
    "2. **Data Transformation**:\n",
    "   - The raw data is processed using User-Defined Functions (UDFs) tailored to extract relevant fields such as position titles, salaries, job requirements, and notes.\n",
    "   - Data cleaning and validation steps are performed to ensure the quality of the output.\n",
    "\n",
    "3. **Real-Time Processing**:\n",
    "   - Apache Spark processes the data streams in real-time, performing transformations and aggregations dynamically.\n",
    "\n",
    "4. **Cloud Integration**:\n",
    "   - The processed data is stored in AWS S3 for scalability, accessibility, and future analytics.\n",
    "   - Parquet is used for storage, ensuring a compact, efficient format.\n",
    "\n",
    "5. **Output**:\n",
    "   - The cleaned and structured data is outputted to the console for debugging and stored in the cloud for future use in visualization tools like Power BI or Tableau.\n",
    "\n",
    "---\n",
    "\n",
    "## **Project Goals**\n",
    "- To develop a fully automated data processing pipeline for real-time ingestion and transformation of job-related data.\n",
    "- To explore and handle a variety of data formats efficiently.\n",
    "- To demonstrate the use of distributed data processing systems like Apache Spark in solving real-world data challenges.\n",
    "- To create a scalable solution that can be extended to other use cases and data sources in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Potential Use Cases**\n",
    "1. Recruitment Analytics Platform\n",
    "* Automate the extraction of structured data from unstructured job postings in various formats (e.g., text, JSON, PDFs).\n",
    "* Provide insights into salary benchmarks, job demand trends, and required qualifications using dashboards.\n",
    "* Help recruiters make data-driven decisions for hiring and resource allocation.\n",
    "\n",
    "2. Resume Parsing for Applicant Tracking Systems (ATS)\n",
    "* Extract essential candidate information like skills, experience, and education.\n",
    "* Enable companies to match resumes to job postings automatically, enhancing the recruitment process.\n",
    "\n",
    "3. Market Research on Job Trends\n",
    "* Analyze job postings to identify emerging roles, skills, and industries.\n",
    "* Provide valuable insights for training organizations and career development platforms to align their offerings with market demand.\n",
    "* Help policymakers understand labor market trends to shape educational and workforce policies.\n",
    "\n",
    "4. Real-Time Job Alert System\n",
    "* Process job postings in real-time to send customized alerts to users.\n",
    "* Allow filtering by criteria such as salary range, location, or job title.\n",
    "* Improve job-seeking platforms by offering timely and relevant notifications.\n",
    "\n",
    "5. Job Posting Validation and Quality Control\n",
    "* Automate the validation of job descriptions to ensure compliance with regulatory standards and organizational guidelines (e.g., salary transparency, inclusive language).\n",
    "* Enhance the quality and consistency of postings on job boards by identifying incomplete or non-compliant descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** This project was specifically designed and tested for processing JSON and text file formats. While the code contains provisions for handling additional formats such as CSV, PDF, Excel, and images, these functionalities are currently untested and may require further development and validation. Users intending to use this code for untested formats are advised to thoroughly debug and validate the processing steps for these formats to ensure accuracy and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **System Architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Necessary Modules**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import udf, regexp_replace, input_file_name\n",
    "from config import configuration  # Configuration file for credentials\n",
    "from udf_utils import *  # Utility functions for UDFs\n",
    "from tika import parser  # For processing PDF files\n",
    "import pytesseract  # For processing image files\n",
    "from PIL import Image\n",
    "import pandas as pd  # For reading Excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining Functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_udfs():\n",
    "    \"\"\"\n",
    "    This function defines and returns User-Defined Functions (UDFs) for various data extraction tasks.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'extract_file_name_udf': udf(extract_file_name, StringType()),\n",
    "        'extract_position_udf': udf(extract_position, StringType()),\n",
    "        'extract_salary_udf': udf(extract_salary, StructType([\n",
    "            StructField('salary_start', DoubleType(), True),\n",
    "            StructField('salary_end', DoubleType(), True)\n",
    "        ])),\n",
    "        'extract_start_date_udf': udf(extract_start_date, DateType()),\n",
    "        'extract_end_date_udf': udf(extract_end_date, DateType()),\n",
    "        'extract_classcode_udf': udf(extract_class_code, StringType()),\n",
    "        'extract_requirements_udf': udf(extract_requirements, StringType()),\n",
    "        'extract_notes_udf': udf(extract_notes, StringType()),\n",
    "        'extract_duties_udf': udf(extract_duties, StringType()),\n",
    "        'extract_selection_udf': udf(extract_selection, StringType()),\n",
    "        'extract_experience_length_udf': udf(extract_experience_length, StringType()),\n",
    "        'extract_education_length_udf': udf(extract_education_length, StringType()),\n",
    "        'extract_application_location_udf': udf(extract_application_location, StringType()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text content from a PDF file.\n",
    "    \"\"\"\n",
    "    raw_text = parser.from_file(file_path)\n",
    "    return raw_text['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from an image using OCR (Optical Character Recognition).\n",
    "    \"\"\"\n",
    "    image = Image.open(file_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_excel(file_path):\n",
    "    \"\"\"\n",
    "    Reads content from an Excel file and converts it to a Spark DataFrame.\n",
    "    \"\"\"\n",
    "    pandas_df = pd.read_excel(file_path)\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark_df = spark.createDataFrame(pandas_df)\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main Execution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize Spark Session\n",
    "    spark = (\n",
    "        SparkSession.builder.appName('Real-Time Job Data Streaming')\n",
    "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk:1.11.469')\n",
    "        .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "        .config('spark.hadoop.fs.s3a.access.key', configuration.get('AWS_ACCESS_KEY'))  # AWS Access Key\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', configuration.get('AWS_SECRET_KEY'))  # AWS Secret Key\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Define input directories for different data formats\n",
    "    text_input_dir = 'input_text'\n",
    "    json_input_dir = 'input_json'\n",
    "    csv_input_dir = 'input_csv'\n",
    "    pdf_input_dir = 'input_pdf'\n",
    "    img_input_dir = 'input_img'\n",
    "    excel_input_dir = 'input_excel'\n",
    "\n",
    "    # Define schema for structured data\n",
    "    data_schema = StructType([\n",
    "        StructField('file_name', StringType(), True),\n",
    "        StructField('position', StringType(), True),\n",
    "        StructField('classcode', StringType(), True),\n",
    "        StructField('salary_start', DoubleType(), True),\n",
    "        StructField('salary_end', DoubleType(), True),\n",
    "        StructField('start_date', DateType(), True),\n",
    "        StructField('end_date', DateType(), True),\n",
    "        StructField('req', StringType(), True),\n",
    "        StructField('notes', StringType(), True),\n",
    "        StructField('duties', StringType(), True),\n",
    "        StructField('selection', StringType(), True),\n",
    "        StructField('experience_length', StringType(), True),\n",
    "        StructField('job_type', StringType(), True),\n",
    "        StructField('education_length', StringType(), True),\n",
    "        StructField('school_type', StringType(), True),\n",
    "        StructField('application_location', StringType(), True),\n",
    "    ])\n",
    "\n",
    "    # Load UDFs\n",
    "    udfs = define_udfs()\n",
    "\n",
    "    # Process text input as a stream\n",
    "    job_bulletins_df = (\n",
    "        spark.readStream.format('text')\n",
    "        .option('wholetext', 'true')\n",
    "        .load(text_input_dir)\n",
    "    )\n",
    "\n",
    "    # Apply transformations using UDFs\n",
    "    job_bulletins_df = (\n",
    "        job_bulletins_df\n",
    "        .withColumn('file_name', udfs['extract_file_name_udf']('value'))\n",
    "        .withColumn('position', udfs['extract_position_udf']('value'))\n",
    "        .withColumn('salary_start', udfs['extract_salary_udf']('value').getField('salary_start'))\n",
    "        .withColumn('salary_end', udfs['extract_salary_udf']('value').getField('salary_end'))\n",
    "        .withColumn('start_date', udfs['extract_start_date_udf']('value'))\n",
    "        .withColumn('end_date', udfs['extract_end_date_udf']('value'))\n",
    "        .withColumn('classcode', udfs['extract_classcode_udf']('value'))\n",
    "        .withColumn('req', udfs['extract_requirements_udf']('value'))\n",
    "        .withColumn('notes', udfs['extract_notes_udf']('value'))\n",
    "        .withColumn('duties', udfs['extract_duties_udf']('value'))\n",
    "        .withColumn('selection', udfs['extract_selection_udf']('value'))\n",
    "        .withColumn('experience_length', udfs['extract_experience_length_udf']('value'))\n",
    "        .withColumn('education_length', udfs['extract_education_length_udf']('value'))\n",
    "        .withColumn('application_location', udfs['extract_application_location_udf']('value'))\n",
    "    )\n",
    "\n",
    "    # Process JSON input as a stream\n",
    "    json_df = (\n",
    "        spark.readStream.format('json')\n",
    "        .schema(data_schema)\n",
    "        .load(json_input_dir)\n",
    "    )\n",
    "\n",
    "    # Process PDF files\n",
    "    pdf_files = [process_pdf(f'{pdf_input_dir}/{file}') for file in os.listdir(pdf_input_dir)]\n",
    "\n",
    "    # Process image files\n",
    "    img_files = [process_image(f'{img_input_dir}/{file}') for file in os.listdir(img_input_dir)]\n",
    "\n",
    "    # Process Excel files\n",
    "    excel_dfs = [process_excel(f'{excel_input_dir}/{file}') for file in os.listdir(excel_input_dir)]\n",
    "\n",
    "    # Union DataFrames\n",
    "    combined_df = job_bulletins_df.union(json_df)\n",
    "\n",
    "    # Function for writing stream to Parquet\n",
    "    def stream_writer(input_df: DataFrame, checkpoint_dir: str, output_dir: str):\n",
    "        \"\"\"\n",
    "        Writes the streaming DataFrame to a Parquet file with checkpointing.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            input_df.writeStream.format('parquet')\n",
    "            .option('checkpointLocation', checkpoint_dir)\n",
    "            .option('path', output_dir)\n",
    "            .outputMode('append')\n",
    "            .trigger(processingTime='5 seconds')\n",
    "            .start()\n",
    "        )\n",
    "\n",
    "    # Write the output to S3\n",
    "    query = stream_writer(\n",
    "        combined_df,\n",
    "        checkpoint_dir='s3a://sparkunstructuredstreaming/checkpoints/',\n",
    "        output_dir='s3a://sparkunstructuredstreaming/data/real_time_streaming/'\n",
    "    )\n",
    "\n",
    "    # Write the output to the console for debugging\n",
    "    query_console = (\n",
    "        combined_df.writeStream.outputMode('append')\n",
    "        .format('console')\n",
    "        .option('truncate', False)\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    # Await termination\n",
    "    query_console.awaitTermination()\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
